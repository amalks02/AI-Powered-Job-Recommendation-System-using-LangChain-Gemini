{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxLajhJznK77"
      },
      "outputs": [],
      "source": [
        "# ------------------ INSTALL DEPENDENCIES ------------------\n",
        "!pip install streamlit PyPDF2 faiss-cpu sentence-transformers python-dotenv google-generativeai pyngrok -q\n",
        "\n",
        "# ------------------ STREAMLIT APP FILE ------------------\n",
        "streamlit_code = \"\"\"\n",
        "import os\n",
        "import faiss\n",
        "import numpy as np\n",
        "import PyPDF2\n",
        "import streamlit as st\n",
        "from dotenv import load_dotenv\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
        "\n",
        "# Initialize embedder\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# ---------------- PDF Processing ----------------\n",
        "def get_pdf_text(pdf_docs):\n",
        "    \\\"\\\"\\\"Extract text from uploaded PDFs.\\\"\\\"\\\"\n",
        "    text = \"\"\n",
        "    for pdf in pdf_docs:\n",
        "        try:\n",
        "            pdf_reader = PyPDF2.PdfReader(pdf)\n",
        "            for page in pdf_reader.pages:\n",
        "                text += page.extract_text() or \"\"\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error reading {pdf.name}: {e}\")\n",
        "    return text\n",
        "\n",
        "def split_text(text, chunk_size=500, overlap=50):\n",
        "    \\\"\\\"\\\"Split text into overlapping word chunks.\\\"\\\"\\\"\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    for i in range(0, len(words), chunk_size - overlap):\n",
        "        chunk = \" \".join(words[i:i + chunk_size])\n",
        "        chunks.append(chunk)\n",
        "    return chunks\n",
        "\n",
        "# ---------------- FAISS Indexing ----------------\n",
        "@st.cache_resource\n",
        "def create_vector_store(chunks):\n",
        "    \\\"\\\"\\\"Create and cache FAISS index for text chunks.\\\"\\\"\\\"\n",
        "    embeddings = embedder.encode(chunks)\n",
        "    dim = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dim)\n",
        "    index.add(np.array(embeddings).astype('float32'))\n",
        "    return index, chunks\n",
        "\n",
        "def search_chunks(index, chunks, query, top_k=3):\n",
        "    \\\"\\\"\\\"Retrieve top matching chunks for a query.\\\"\\\"\\\"\n",
        "    query_embedding = embedder.encode([query])\n",
        "    D, I = index.search(np.array(query_embedding).astype('float32'), k=top_k)\n",
        "    return [chunks[i] for i in I[0]]\n",
        "\n",
        "# ---------------- Gemini Query ----------------\n",
        "def generate_answer(context, question):\n",
        "    \\\"\\\"\\\"Generate an answer using Gemini model.\\\"\\\"\\\"\n",
        "    full_prompt = f\\\"\\\"\\\"\n",
        "    Answer the question using only the information provided in the context.\n",
        "    Be accurate and detailed. If the answer is not in the context, say:\n",
        "    'The answer is not provided in the context.'\n",
        "\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Question:\n",
        "    {question}\n",
        "\n",
        "    Answer:\n",
        "    \\\"\\\"\\\"\n",
        "    model = genai.GenerativeModel(model_name=\"gemini-2.5-flash\")\n",
        "    response = model.generate_content(full_prompt)\n",
        "    return response.text.strip()\n",
        "\n",
        "# ---------------- Streamlit UI ----------------\n",
        "def main():\n",
        "    st.set_page_config(\"Chat with PDF - Gemini 2.5\", layout=\"wide\")\n",
        "    st.title(\"üìÑ Chat with your PDF using Gemini 2.5 Flash\")\n",
        "\n",
        "    with st.sidebar:\n",
        "        st.header(\"üì§ Upload PDFs\")\n",
        "        pdf_docs = st.file_uploader(\"Upload PDF files\", type=[\"pdf\"], accept_multiple_files=True)\n",
        "\n",
        "        if st.button(\"üìö Process PDFs\"):\n",
        "            if not pdf_docs:\n",
        "                st.warning(\"Please upload at least one PDF file.\")\n",
        "            else:\n",
        "                with st.spinner(\"Reading and indexing...\"):\n",
        "                    raw_text = get_pdf_text(pdf_docs)\n",
        "                    if not raw_text.strip():\n",
        "                        st.error(\"No extractable text found in the uploaded PDFs.\")\n",
        "                    else:\n",
        "                        chunks = split_text(raw_text)\n",
        "                        index, chunk_list = create_vector_store(chunks)\n",
        "                        st.session_state.index = index\n",
        "                        st.session_state.chunks = chunk_list\n",
        "                        st.success(\"‚úÖ PDFs processed! You can now ask questions.\")\n",
        "\n",
        "    user_question = st.text_input(\"üí¨ Ask a question about your PDFs:\")\n",
        "    if user_question:\n",
        "        if \"index\" not in st.session_state:\n",
        "            st.warning(\"Please upload and process a PDF first.\")\n",
        "        else:\n",
        "            with st.spinner(\"Thinking...\"):\n",
        "                top_chunks = search_chunks(st.session_state.index, st.session_state.chunks, user_question)\n",
        "                context_text = \" \".join(top_chunks)\n",
        "                answer = generate_answer(context_text, user_question)\n",
        "                st.subheader(\"üìù Answer\")\n",
        "                st.write(answer)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\"\"\"\n",
        "\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(streamlit_code)\n",
        "\n",
        "# ------------------ RUN STREAMLIT + NGROK ------------------\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Set your Gemini API key here in Colab - Replace with your actual key or use Colab Secrets\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_GEMINI_API_KEY\" # Consider using Colab secrets for this\n",
        "\n",
        "# Start ngrok tunnel\n",
        "# Replace YOUR_AUTHTOKEN with your actual ngrok authtoken if you have one for persistent URLs\n",
        "# ngrok.set_auth_token(\"YOUR_AUTHTOKEN\") # Uncomment and replace if needed\n",
        "ngrok.set_auth_token(\"315HtXLPcxhoHCUGkFoRzHEJqXI_7vZJiGch18iDAc5d62pYU\") # ADD YOUR NGROK AUTHTOKEN HERE\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"Streamlit app is running here: {public_url}\")\n",
        "\n",
        "# Run Streamlit\n",
        "!streamlit run app.py --server.port 8501"
      ]
    }
  ]
}